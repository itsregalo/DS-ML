{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP5046_Lab02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHPkKbuhPF6"
      },
      "source": [
        "# Lab 02\n",
        "\n",
        "Today we will investigate some word representation models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "qZ0HLTPGemjV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNyhgK5QTOuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b607b16-334c-47f3-8d12-2b0876a2b8eb"
      },
      "source": [
        "import pprint\n",
        "import re\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmae7urS8RHD"
      },
      "source": [
        "### Download data from Google Drive\n",
        "For today's lab we will download and use the TED script data from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7vMHSahdnf"
      },
      "source": [
        "#### Google Drive Access Setup\n",
        "Running the following code will generate a link and a field for entering a verification code.\n",
        "\n",
        "Click the link, which will direct to the Google Sign In page. Sign in with your own Google account by following the instructions on the page.\n",
        "\n",
        "Then copy the generated verification code from the page into the verification code field and press Enter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTSQtnPkfyzj"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewAbjQzThnT5"
      },
      "source": [
        "#### Downloading TED Scripts from Google Drive \n",
        "Click on left side \"Files\" tab and see the file is downloaded successfully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVk7tjwvhl-6"
      },
      "source": [
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIPpEvI4kqMV"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYmEQgB7XoDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5199f151-286f-49f0-fed1-7b7b6beeac5d"
      },
      "source": [
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Get the contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Remove \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenise the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Remove punctuation and change all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenise each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "print(sentences[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CojV1MbhkQxK"
      },
      "source": [
        "### Word2Vec - Continuous Bag-Of-Words (CBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLq1VIZ7TDog"
      },
      "source": [
        "For more details about gensim.models.word2vec you can refer to [API for Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW1iEee3lZC9"
      },
      "source": [
        "# Initialize and train a word2vec model with the following parameters:\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "# size: dimensionality of the word vectors\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "# workers: Use specified number of worker threads to train the model (=faster training with multicore machines)\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FKp3X7pkRm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc912be-57ac-408e-c111-d58dc0283a96"
      },
      "source": [
        "# The trained word vectors are stored in a KeyedVectors instance as model.wv\n",
        "# Get the top 10 similar words to 'man' by calling most_similar() \n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "\n",
        "similar_words= wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8640294671058655),\n",
            " ('guy', 0.8014923334121704),\n",
            " ('lady', 0.7821930050849915),\n",
            " ('boy', 0.7755894660949707),\n",
            " ('girl', 0.7453110218048096),\n",
            " ('soldier', 0.7424092888832092),\n",
            " ('gentleman', 0.7205733060836792),\n",
            " ('kid', 0.716789186000824),\n",
            " ('john', 0.6657833456993103),\n",
            " ('poet', 0.6656205654144287)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsFHg0znlPSf"
      },
      "source": [
        "### Word2Vec - Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k16AowhCWUXu"
      },
      "source": [
        "# Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8UiVfr2cBtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d151389d-311a-4f06-a658-c4dc8aa22d2e"
      },
      "source": [
        "similar_words = wv_sg_model.wv.most_similar(\"man\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('guy', 0.7679535150527954),\n",
            " ('woman', 0.7646703720092773),\n",
            " ('soldier', 0.7027142643928528),\n",
            " ('boy', 0.6967617273330688),\n",
            " ('michelangelo', 0.6850951910018921),\n",
            " ('lady', 0.6850934624671936),\n",
            " ('adage', 0.6817587018013),\n",
            " ('psychiatrist', 0.6801409721374512),\n",
            " ('rabbi', 0.6777557134628296),\n",
            " ('son', 0.6765754222869873)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfF7YqvpppbG"
      },
      "source": [
        "## Word2Vec vs FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8IV7D6VAEcr"
      },
      "source": [
        "Word2Vec - Skip Gram cannot find similar words to \"electrofishing\" as \"electrofishing\" is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS9c2uWWquWG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "301ada6f-f3c5-48bf-fcca-9dcc89f84629"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a2f7ec57b31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilar_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwv_sg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TpkScI8sA9G"
      },
      "source": [
        "### FastText - Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAqOR1Vqps6M"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqkvyiUw_DRh"
      },
      "source": [
        "# Now we initialize and train FastText with Skip Gram architecture (sg=1)\n",
        "ft_sg_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv26QObJriB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273a86da-a1df-48bf-e309-f6d69b73442d"
      },
      "source": [
        "# As we can see, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result = ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('electrolux', 0.8021794557571411),\n",
            " ('electric', 0.8006105422973633),\n",
            " ('electro', 0.799832284450531),\n",
            " ('electrolyte', 0.7924953699111938),\n",
            " ('electroshock', 0.7730218172073364),\n",
            " ('electrochemical', 0.7646723985671997),\n",
            " ('airbus', 0.7538162469863892),\n",
            " ('gastric', 0.7533923387527466),\n",
            " ('electrified', 0.7487292289733887),\n",
            " ('electrogram', 0.7448490262031555)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0x2aQpfsFSx"
      },
      "source": [
        "### FastText - Continuous Bag-Of-Words (CBOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUBqvqpc2sbL"
      },
      "source": [
        "# Now we initialize and train FastText with CBOW architecture (sg=0)\n",
        "ft_cbow_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUj1RUzM2nLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d439b2-a87c-4b5f-a71a-af47dba82353"
      },
      "source": [
        "# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result = ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('electric', 0.9238611459732056),\n",
            " ('electro', 0.9049367308616638),\n",
            " ('electrolux', 0.8951212763786316),\n",
            " ('electron', 0.8847237825393677),\n",
            " ('electroshock', 0.8807465434074402),\n",
            " ('electronic', 0.8762202262878418),\n",
            " ('electrical', 0.8743261098861694),\n",
            " ('electrolyte', 0.8743162155151367),\n",
            " ('electrode', 0.8685972690582275),\n",
            " ('electromagnet', 0.8603188991546631)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hjmOhmRi7Ov"
      },
      "source": [
        "## King - Man + Woman = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw7b9OSwjGm0"
      },
      "source": [
        "Try both CBOW and Skip Gram model to calculate \"King - Man + Woman = ?\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovTXjSdgrw36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dc23d2-bdd7-48ec-b5bc-9e24f11c5151"
      },
      "source": [
        "# We can specify the positive/negative word list with the positive/negative parameters to create a word expression\n",
        "# Top N most similar words can be specified with the topn parameter\n",
        "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('president', 0.7756727337837219)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUtbE2jwq1to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44d4208-4786-4bc6-b1f3-46664f278324"
      },
      "source": [
        "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('luther', 0.6858599185943604)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PWf2I4_WZpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebce1a7-c9d4-4908-ba2e-7ebaf76fd584"
      },
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('kidding', 0.9025802612304688)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9x51rRhWZrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f45bfe-c3c4-4cdc-94e3-b10a121745cd"
      },
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('jarring', 0.6985799074172974)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpAd8t-wjTMA"
      },
      "source": [
        "This is not what we expected...Probably not enough data to answer as \"queen\"\n",
        "\n",
        "Let's try with a larger sized training data (Google has already trained Word2Vec with Google News data) in the following section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMY5w8F7rElp"
      },
      "source": [
        "## Using Pretrained word embeddings with Gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keivkY13L4Nz"
      },
      "source": [
        "### 1.Download and load from Google pretrained Word2Vec binary file\n",
        "[Link to Project](https://code.google.com/archive/p/word2vec/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teQvZDSirVVC"
      },
      "source": [
        "# Download the pre-trained vectors trained on part of Google News dataset (about 100 billion words)\n",
        "# Beware, this file is big (3.39GB) - might be long waiting! \n",
        "id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "downloaded = drive.CreateFile({'id':id2}) \n",
        "downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTrXl4FRMitm"
      },
      "source": [
        "# Uncompress the downloaded file\n",
        "!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64e_sRJ1rhUa"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Load the pretrained vectors with KeyedVectors instance\n",
        "# Note that we set the limit=100000 here, which means we set a maximum number of word-vectors to read from the file, to avoid out of memory issue and load vectors faster. \n",
        "\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "gn_wv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvMQp2-Tr3zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49285f1-0bbb-4d01-90b9-a00aaf11b288"
      },
      "source": [
        "# Now we can try to calculate \"King - Man + Woman = ?\" again\n",
        "result = gn_wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118192911148071)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also try to extract the word embeddings and check their shape\n",
        "wv_banana = gn_wv_model[\"banana\"] \n",
        "wv_avocado = gn_wv_model[\"avocado\"]\n",
        "print(wv_banana.shape)\n",
        "print(wv_avocado.shape)\n",
        "\n",
        "# We can also calculate the cosine similarity ourselves with the extracted words\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity([wv_banana],[wv_avocado])"
      ],
      "metadata": {
        "id": "O9N64tY4RNlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23909c70-1965-4db5-abfd-b77caa6b7a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n",
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5332662]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Ws7QvPMq9s"
      },
      "source": [
        "### 2.Load a pretrained word embedding model using API\n",
        "The following code illustrates another way of loading pretrained word embeddings with Gensim. Here we try with GloVe embedding trained on Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvAP4nyYM_qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7233b00f-0e71-437f-aafc-92841889e747"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# download the model and return as object ready for use\n",
        "model = api.load(\"glove-twitter-25\")  \n",
        "# The similarity() function can calculate the cosine similarity between two given words\n",
        "print(model.similarity(\"cat\",\"dog\"))\n",
        "# The distance() function is another way of calculating the similarity between two given words, which returns (1 - cosine similarity) instead\n",
        "print(model.distance(\"cat\",\"dog\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "0.95908207\n",
            "0.040917932987213135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqLruu6247Ze"
      },
      "source": [
        "# [Tips] Play with Colab Form Fields \n",
        "**The Form** supports multiple types of fields, including **input fields**, **dropdown menus**. \n",
        "\n",
        "In Lab1 E1, we already used the input fields. Let's try more now. You can edit this section by double-clicking it. \n",
        "\n",
        "Let's get familiar by changing the value in each input field (on the right) and checking the changes in the code (on the left) - and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBNvQmee5QIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbcbdb57-083e-4d6c-d5af-063b27c91395"
      },
      "source": [
        "#@title Example form fields\n",
        "#@markdown please put description\n",
        "\n",
        "string = 'examples'  #@param {type: \"string\"}\n",
        "slider_value = 117  #@param {type: \"slider\", min: 100, max: 200}\n",
        "number = 102  #@param {type: \"number\"}\n",
        "date = '2020-01-05'  #@param {type: \"date\"}\n",
        "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
        "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#print the output\n",
        "print(\"string is\",string)\n",
        "print('slider_value',slider_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "string is examples\n",
            "slider_value 117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrjbuZYrXD88"
      },
      "source": [
        "# Extension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UWjBxLdTcEi"
      },
      "source": [
        "## Word Embedding Visual Inspector (WEVI)\n",
        "If you would like to visualise how Word2Vec is learning, the following link is useful https://ronxin.github.io/wevi/"
      ]
    }
  ]
}