{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to download file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('training_data.csv')  \n",
    "\n",
    "id = '1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('testing_data.csv')  \n",
    "\n",
    "import pandas as pd\n",
    "training_data = pd.read_csv(\"/content/training_data.csv\")\n",
    "testing_data = pd.read_csv(\"/content/testing_data.csv\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
    "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Sample Data\")\n",
    "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data.iloc[-1,0], training_data.iloc[-1,1]))\n",
    "print(\"------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and posts and store into List\n",
    "\n",
    "# Get the list of training data (posts)\n",
    "training_posts=training_data['posts'].tolist()\n",
    "# Get the list of corresponding labels for the training data (posts)\n",
    "training_labels=training_data['type'].tolist()\n",
    "\n",
    "# Get the list of testing data (posts)\n",
    "testing_posts=testing_data['posts'].tolist()\n",
    "# Get the list of corresponding labels for the testing data (posts)\n",
    "testing_labels=testing_data['type'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "training_posts = [remove_url(post) for post in training_posts]\n",
    "testing_posts = [remove_url(post) for post in testing_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are asked to pre-process the training set by integrating several text pre-processing techniques\n",
    " (e.g. tokenisation, removing numbers, converting to lowercase, removing stop words, stemming, etc.).\n",
    "You should test and justify the reason why you apply the specific preprocessing techniques based on the test result in section\n",
    "\"\"\"\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove the stop words\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    # Remove the punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Remove the numbers\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    # Stem the tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # lemmatization \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "clean_training_posts = [preprocess(post) for post in training_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, you are to implement three input representation components, including \n",
    "1) Word Embedding Construction Module, \n",
    "2) Pretrained Word Embedding Module, and \n",
    "3) Input Concatenation Module. For training, you are free to choose hyperparameters [Lab2,Lab4,Lab5] \n",
    "(e.g. dimension of embeddings, learning rate, epochs, etc.).\n",
    "\n",
    "First, you are asked to build a word embedding model (for representing word vectors, \n",
    "such as word2vec-CBOW, word2vec-Skip gram, fastText, and Glove) \n",
    "for the input embedding of your sequence model. \n",
    "Note that we used one-hot vectors as inputs for the sequence model in the Lab3 and Lab4.\n",
    " You are required to complete the following sections in the format\n",
    "\n",
    "Preprocess data for word embeddings: You are to use and preprocess MBTI dataset \n",
    "(the one provided in the Section 1) for training word embeddings [Lab2]. \n",
    "This can be different from the preprocessing technique that you used in Section 1. \n",
    "You can use both the training and testing datasets in order to train the word embedding.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess data for word embeddings\n",
    "clean_training_posts = [preprocess(post) for post in training_posts]\n",
    "clean_testing_posts = [preprocess(post) for post in testing_posts]\n",
    "\n",
    "\"\"\"\n",
    " You are to build a training model for word embeddings. \n",
    " You are required to articulate the hyperparameters you choose (dimension of embeddings and window size) in the report.\n",
    " Note that any word embeddings model (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) can be applied.\n",
    "\"\"\"\n",
    "\n",
    "# Build the training model for word embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "window_size = 5\n",
    "vocab_size = len(clean_training_posts)\n",
    "\n",
    "\n",
    "# Build the word embedding training model\n",
    "class TrainingModelForWordEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, window_size):\n",
    "        super(TrainingModelForWordEmbeddings, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim * window_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, self.embedding_dim * self.window_size)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build the word embedding testing model\n",
    "model = TrainingModelForWordEmbeddings(embedding_dim, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are asked to extract and apply the pretrained word embedding. Gensim provides several pretrained word embeddings, \n",
    "you can find those in the gensim github. You can select the pretrained word embedding that would be useful for the assignment 1 task,\n",
    " personality type classification\n",
    "\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# extract and apply the pretrained word embedding\n",
    "import gensim.downloader as api\n",
    "# select the pretrained word embedding for Predicting-Myers-Briggs-Type-Indicator-with-Recurrent-Neural-Networks\n",
    "word_embedding_model = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb_dim = word_embedding_model.vector_size\n",
    "\n",
    "emb_table = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if word in word_embedding_model:\n",
    "        emb_table.append(word_embedding_model[word])\n",
    "    else:\n",
    "        # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined\n",
    "        # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity\n",
    "        emb_table.append([0]*emb_dim)\n",
    "emb_table = np.array(emb_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the trained word embedding and pretrained word embedding,[Lab5] and apply to the sequence model \n",
    "class InputConcatenationModule(nn.Module):\n",
    "    def __init__(self, embedding_dim, window_size, pretrained_embedding):\n",
    "        super(InputConcatenationModule, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.pretrained_embedding = pretrained_embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim * window_size + embedding_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, self.embedding_dim * self.window_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = torch.nn.utils.rnn.pad_sequence\n",
    "word_index = {word: i for i, word in enumerate(pretrained_embedding.vocab)}\n",
    "\n",
    "def encode_and_add_padding(sentences, seq_length, word_index):\n",
    "    sent_encoded = []\n",
    "    for sentence in sentences:\n",
    "        encoded = [word_index[word] for word in sentence.split()]\n",
    "        sent_encoded.append(encoded)\n",
    "    sent_encoded = pad_sequences(sent_encoded, maxlen=seq_length, padding='post')\n",
    "    return sent_encoded\n",
    "\n",
    "train_pad_encoded = encode_and_add_padding(clean_training_posts, seq_length=100, word_index=word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the training model for word embeddings\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, window_size, vocab_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim * window_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, self.embedding_dim * self.window_size)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = Model(embedding_dim, window_size, vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_embeddings, test_embeddings, train_label, test_label = train_test_split(pretrained_embedding, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# The length of each sequence\n",
    "sequence_length = len(X_train[0])\n",
    "# The input feature dimension\n",
    "input_dim = len(X_train[0][0])\n",
    "# The number of class\n",
    "n_class = len(np.unique(y_train))\n",
    "\n",
    "# Set the hyperparameters \n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "n_hidden = 128\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize model, set up the loss calculator and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for ind in range(0, len(X_train), batch_size):\n",
    "        batch_x = X_train[ind:ind+batch_size]\n",
    "        batch_y = y_train[ind:ind+batch_size]\n",
    "        batch_x = torch.from_numpy(np.array(batch_x)).long()\n",
    "        batch_y = torch.from_numpy(np.array(batch_y)).long()\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print('Epoch:', epoch, 'Loss:', train_loss)\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.NLLLoss()\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(torch.tensor(clean_training_posts).long().to(device))\n",
    "    loss = loss_function(output, torch.tensor(training_labels).long().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item()}')\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Sequence Model (Bi-directional model)\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, window_size):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, bidirectional=True)\n",
    "        self.linear = nn.Linear(embedding_dim * 2, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, self.embedding_dim * self.window_size)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build the sequence model\n",
    "model = SequenceModel(embedding_dim, window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Sequence Model (Bi-directional model)\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(clean_training_posts), batch_size):\n",
    "        batch_inputs = clean_training_posts[i:i+batch_size]\n",
    "        batch_labels = training_labels[i:i+batch_size]\n",
    "        batch_inputs = torch.tensor(batch_inputs)\n",
    "        batch_labels = torch.tensor(batch_labels)\n",
    "        batch_inputs = batch_inputs.long()\n",
    "        batch_labels = batch_labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the \n",
    "result of Semantic-Syntactic word relationship tests.\n",
    "\"\"\"\n",
    "# test the trained word embedding\n",
    "def test_word_embedding(model, test_posts, test_labels):\n",
    "    test_posts = torch.tensor(test_posts)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_posts = test_posts.long()\n",
    "    test_labels = test_labels.long()\n",
    "    outputs = model(test_posts)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == test_labels).sum().item()\n",
    "    return correct / len(test_labels)\n",
    "\n",
    "# test the trained word embedding\n",
    "print('Test accuracy: {}'.format(test_word_embedding(model, clean_testing_posts, testing_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with the testing dataset and provide the table with f1 of test set\n",
    "def f1_score_table(model, test_posts, test_labels):\n",
    "    test_posts = torch.tensor(test_posts)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_posts = test_posts.long()\n",
    "    test_labels = test_labels.long()\n",
    "    outputs = model(test_posts)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    f1_score = f1_score(test_labels, predicted, average='macro')\n",
    "    return f1_score\n",
    "\n",
    "print('F1 score: {}'.format(f1_score_table(model, clean_testing_posts, testing_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation with Different Sequence Models\n",
    "# Build the sequence model\n",
    "model = SequenceModel(embedding_dim, window_size)\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(clean_training_posts), batch_size):\n",
    "        batch_inputs = clean_training_posts[i:i+batch_size]\n",
    "        batch_labels = training_labels[i:i+batch_size]\n",
    "        batch_inputs = torch.tensor(batch_inputs)\n",
    "        batch_labels = torch.tensor(batch_labels)\n",
    "        batch_inputs = batch_inputs.long()\n",
    "        batch_labels = batch_labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "# test the trained word embedding\n",
    "print('Test accuracy: {}'.format(test_word_embedding(model, clean_testing_posts, testing_labels)))\n",
    "# evaluate with the testing dataset and provide the table with f1 of test set\n",
    "print('F1 score: {}'.format(f1_score_table(model, clean_testing_posts, testing_labels)))\n",
    "\n",
    "f1_score = f1_score_table(model, clean_testing_posts, testing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a graph of the f1 score of the testing set against the number of epochs\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs = range(1, num_epochs + 1)\n",
    "plt.plot(range(num_epochs), f1_score_table(model, clean_testing_posts, testing_labels))\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Personality Type Prediction\n",
    "\n",
    "text = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# predict the input text's personality type using the trained word embedding\n",
    "def predict_personality_type(model, text):\n",
    "    text = torch.tensor(text)\n",
    "    text = text.long()\n",
    "    outputs = model(text)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "print('Personality Type: {}'.format(predict_personality_type(model, text)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
