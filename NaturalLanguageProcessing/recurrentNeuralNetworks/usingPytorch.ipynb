{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to download file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('training_data.csv')  \n",
    "\n",
    "id = '1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('testing_data.csv')  \n",
    "\n",
    "import pandas as pd\n",
    "training_data = pd.read_csv(\"/content/training_data.csv\")\n",
    "testing_data = pd.read_csv(\"/content/testing_data.csv\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
    "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Sample Data\")\n",
    "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data.iloc[-1,0], training_data.iloc[-1,1]))\n",
    "print(\"------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and posts and store into List\n",
    "\n",
    "# Get the list of training data (posts)\n",
    "training_posts=training_data['posts'].tolist()\n",
    "# Get the list of corresponding labels for the training data (posts)\n",
    "training_labels=training_data['type'].tolist()\n",
    "\n",
    "# Get the list of testing data (posts)\n",
    "testing_posts=testing_data['posts'].tolist()\n",
    "# Get the list of corresponding labels for the testing data (posts)\n",
    "testing_labels=testing_data['type'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "training_posts = [remove_url(post) for post in training_posts]\n",
    "testing_posts = [remove_url(post) for post in testing_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are asked to pre-process the training set by integrating several text pre-processing techniques\n",
    " (e.g. tokenisation, removing numbers, converting to lowercase, removing stop words, stemming, etc.).\n",
    "You should test and justify the reason why you apply the specific preprocessing techniques based on the test result in section\n",
    "\"\"\"\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove the stop words\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    # Remove the punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Remove the numbers\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    # Stem the tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # tokens to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "clean_training_posts = [preprocess(post) for post in training_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, you are to implement three input representation components, including \n",
    "1) Word Embedding Construction Module, \n",
    "2) Pretrained Word Embedding Module, and \n",
    "3) Input Concatenation Module. For training, you are free to choose hyperparameters [Lab2,Lab4,Lab5] \n",
    "(e.g. dimension of embeddings, learning rate, epochs, etc.).\n",
    "\n",
    "First, you are asked to build a word embedding model (for representing word vectors, \n",
    "such as word2vec-CBOW, word2vec-Skip gram, fastText, and Glove) \n",
    "for the input embedding of your sequence model. \n",
    "Note that we used one-hot vectors as inputs for the sequence model in the Lab3 and Lab4.\n",
    " You are required to complete the following sections in the format\n",
    "\n",
    "Preprocess data for word embeddings: You are to use and preprocess MBTI dataset \n",
    "(the one provided in the Section 1) for training word embeddings [Lab2]. \n",
    "This can be different from the preprocessing technique that you used in Section 1. \n",
    "You can use both the training and testing datasets in order to train the word embedding.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess data for word embeddings\n",
    "clean_training_posts = [preprocess(post) for post in training_posts]\n",
    "clean_testing_posts = [preprocess(post) for post in testing_posts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained word embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data, use clean_training_posts and clean_testing_posts\n",
    "# Load the data, use clean_training_posts and clean_testing_posts\n",
    "X_train = clean_training_posts\n",
    "X_test = clean_testing_posts\n",
    "y_train = training_labels\n",
    "y_test = testing_labels\n",
    "\n",
    "# Convert the labels to one-hot vectors\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# input concatenation\n",
    "def input_concatenation(input_1, input_2):\n",
    "    return torch.cat((input_1, input_2), dim=1)\n",
    "\n",
    "# word embedding construction\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim * 2, 4)\n",
    "    def forward(self, input_1, input_2):\n",
    "        embedded_1 = self.embedding(input_1).view(1, -1)\n",
    "        embedded_2 = self.embedding(input_2).view(1, -1)\n",
    "        concatenated = input_concatenation(embedded_1, embedded_2)\n",
    "        out = self.linear(concatenated)\n",
    "        return out\n",
    "\n",
    "# Build Sequence Model (Bi-directional model)\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, 4)\n",
    "    def forward(self, input_1, input_2):\n",
    "        embedded_1 = self.embedding(input_1).view(1, -1, embedding_dim)\n",
    "        embedded_2 = self.embedding(input_2).view(1, -1, embedding_dim)\n",
    "        output_1, hidden_1 = self.lstm(embedded_1)\n",
    "        output_2, hidden_2 = self.lstm(embedded_2)\n",
    "        concatenated = input_concatenation(output_1, output_2)\n",
    "        out = self.linear(concatenated)\n",
    "        return out\n",
    "\n",
    "# Build the model\n",
    "model = SequenceModel(vocab_size, embedding_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " extract and apply the pretrained word embedding. \n",
    " Gensim provides several pretrained word embeddings, you can find those in the gensim github.\n",
    "  You can select the pretrained word embedding that would be useful personality type classification\n",
    "\"\"\"\n",
    "\n",
    "# extract and apply the pretrained word embedding useful personality type classification\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "word_embedding = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word_embedding.most_similar(positive=['thinking', 'feeling'])\n",
    "\n",
    "vocab_size = len(word_embedding.vocab)\n",
    "embedding_dim = word_embedding.vector_size\n",
    "vocab = word_embedding.vocab\n",
    "hidden_dim = 100\n",
    "\n",
    "# apply the pretrained word embedding\n",
    "def apply_pretrained_embedding(word_embedding, vocab):\n",
    "    embedding_matrix = torch.zeros(vocab_size, embedding_dim)\n",
    "    for word, i in vocab.items():\n",
    "        try:\n",
    "            embedding_vector = word_embedding[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i] = torch.rand(embedding_dim)\n",
    "    return embedding_matrix\n",
    "\n",
    "# apply the pretrained word embedding\n",
    "embedding_matrix = apply_pretrained_embedding(word_embedding, vocab)\n",
    "\n",
    "# Build the model\n",
    "model = SequenceModel(vocab_size, embedding_dim, hidden_dim)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
