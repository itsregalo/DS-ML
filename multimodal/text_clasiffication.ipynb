{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification model based on BERT and LSTM using UPMC-food-101 dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = ['image_path', 'text', 'food']\n",
    "training_data = pd.read_csv('../data/upmc-food-101/train.csv', names=colums, sep=',', index_column=['image_path'])\n",
    "testing_data = pd.read_csv('../data/upmc-food-101/test.csv', names=colums, sep=',', index_column=['image_path'])\n",
    "\n",
    "# sort values by image_path\n",
    "training_data = training_data.sort_values(by=['image_path'])\n",
    "testing_data = testing_data.sort_values(by=['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shape\n",
    "print('Training data shape:', training_data.shape)\n",
    "print('Testing data shape:', testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# clean data function\n",
    "def clean_data(data):\n",
    "    # remove html tags\n",
    "    data = remove_tags(data)\n",
    "    # remove punctuation\n",
    "    data = re.sub(r'[^\\w\\s]', '', data)\n",
    "    # remove numbers\n",
    "    data = re.sub(r'\\d+', '', data)\n",
    "    # remove multiple spaces\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "    # lower case\n",
    "    data = data.lower()\n",
    "    return data\n",
    "\n",
    "# remove tags\n",
    "tags = re.compile(r'<[^>]+>')\n",
    "def remove_tags(data):\n",
    "    return tags.sub('', data)\n",
    "\n",
    "# vectorize data function\n",
    "vectorize_data = np.vectorize(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of classes\n",
    "no_classes = training_data.food.nunique()\n",
    "print('Number of classes:', no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "processed_training_data = vectorize_data(training_data.text.values)\n",
    "processed_testing_data = vectorize_data(testing_data.text.values)\n",
    "\n",
    "encoded_training_labels = encoder.fit_transform(training_data.food.values)\n",
    "encoded_testing_labels = encoder.fit_transform(testing_data.food.values)\n",
    "\n",
    "training_labels = utils.to_categorical(encoded_training_labels, no_classes)\n",
    "testing_labels = utils.to_categorical(encoded_testing_labels, no_classes)\n",
    "\n",
    "print(\"Processed text sample:\", processed_training_data[0])\n",
    "print(\"Shape of train labels:\", training_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "\n",
    "# Import the BERT BASE model from Tensorflow HUB (layer, vocab_file and tokenizer)\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of texts according to BERT\n",
    "def get_masks(text, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    if len(tokens) > max_seq_length:\n",
    "        tokens = tokens[:max_seq_length]\n",
    "    return np.asarray([1] * len(tokens) + [0] * (max_seq_length - len(tokens)), dtype=np.int32)\n",
    "get_masks_vector = np.vectorize(get_masks)\n",
    "\n",
    "def get_segments(text, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    if len(tokens) > max_seq_length:\n",
    "        tokens = tokens[:max_seq_length]\n",
    "\n",
    "    segments_ids = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments_ids.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return np.asarray(segments_ids + [0] * (max_seq_length - len(tokens)), dtype=np.int32)\n",
    "get_segments_vector = np.vectorize(get_segments)\n",
    "\n",
    "def get_ids(text, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from 0 to vocab_size\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    if len(tokens) > max_seq_length:\n",
    "        tokens = tokens[:max_seq_length]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return np.asarray(token_ids + [0] * (max_seq_length - len(token_ids)), dtype=np.int32)\n",
    "get_ids_vector = np.vectorize(get_ids)\n",
    "\n",
    "def prepare(text_array, tokenizer, max_seq_length=128):\n",
    "    \"\"\"Prepare the text samples for BERT\"\"\"\n",
    "    input_ids = get_ids_vector(text_array,tokenizer, max_seq_length).squeeze()\n",
    "    input_masks = get_masks_vector(text_array, max_seq_length).squeeze()\n",
    "    input_segments = get_segments_vector(text_array, max_seq_length).squeeze()\n",
    "    return [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max sequence length according to data\n",
    "max_seq_length = 1313\n",
    "\n",
    "input_ids_train, input_masks_train, input_segments_train = prepare(processed_training_data, tokenizer, max_seq_length)\n",
    "input_ids_test, input_masks_test, input_segments_test = prepare(processed_testing_data, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "segment_ids = layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='segment_ids')\n",
    "den_output, seq_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification model\n",
    "X = layers.LSTM(units=128, return_sequences=True)(seq_output)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "X = layers.Dense(256, activation='relu')(X)\n",
    "X = layers.Dropout(0.5)(X)\n",
    "output = layers.Dense(no_classes, activation='softmax')(X)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer with learning rate of 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set callback for saving the model, log and early stopping conditions\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint = callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "logger = callbacks.CSVLogger('training_1/log.csv')\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([input_ids_train, input_masks_train, input_segments_train], training_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=([input_ids_test, input_masks_test, input_segments_test], testing_labels),\n",
    "                    callbacks=[early_stopping, checkpoint, logger, reduce_lr])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the log file\n",
    "log_df = pd.read_csv('training_1/log.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and Testing accuracy\n",
    "training_accuracy = log_df['acc'].values\n",
    "testing_accuracy = log_df['val_acc'].values\n",
    "\n",
    "# plotting the accuracy\n",
    "plt.plot(training_accuracy, label='Training Accuracy')\n",
    "plt.plot(testing_accuracy, label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test loss\n",
    "training_loss = log_df['loss'].values\n",
    "testing_loss = log_df['val_loss'].values\n",
    "\n",
    "# plotting the loss\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(testing_loss, label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "model.evaluate([input_ids_test, input_masks_test, input_segments_test], testing_labels, batch_size=512)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
