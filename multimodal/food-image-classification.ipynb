{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, MaxPooling1D, GlobalAveragePooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau \n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K \n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.regularizers import l2, l1\n",
    "import cv2\n",
    "from keras.callbacks import CSVLogger\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_directory = '/content/images/train'\n",
    "validation_data_directory = '/content/images/test'\n",
    "nb_train_samples = 67988 \n",
    "nb_validation_samples = 22716\n",
    "n_classes = 101\n",
    "epochs = 10\n",
    "batch_size = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model that enable the freezing of the resnet layers\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "x = base_model.output\n",
    "x = AveragePooling2D(pool_size=(8, 8))(x)\n",
    "x = Dropout(.4)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(n_classes,\n",
    "                    kernel_regularizer=l2(0.005),\n",
    "                    activity_regularizer=l1(0.005), \n",
    "                    activation='softmax')(x)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model to enable the freezing of the resnet layers\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "\n",
    "X = base_model.output\n",
    "X = AveragePooling2D(pool_size=(8, 8))(X)\n",
    "X = Dropout(.4)(X)\n",
    "X = Flatten()(X)\n",
    "\n",
    "predictions = Dense(n_classes,\n",
    "                    kernel_regularizer=l2(0.005),\n",
    "                    activity_regularizer=l1(0.005),\n",
    "                    activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test Data Generators with image augmentation \n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_data_generator.flow_from_directory(\n",
    "    training_data_directory,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_data_generator.flow_from_directory(\n",
    "    validation_data_directory,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the checkpoint\n",
    "checkpoint_filepath = './models/food-image-classification-{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "logger = CSVLogger('./models/training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce learning rate if validation loss does not improve after 3 epochs\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                            factor=0.1, \n",
    "                            patience=3, \n",
    "                            verbose=1, \n",
    "                            mode='auto', \n",
    "                            min_delta=0.0001,\n",
    "                            cooldown=0, \n",
    "                            min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[checkpoint, logger, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log file\n",
    "log_df = pd.read_csv('./models/training.log', sep=',', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test accuracy\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=log_df[0], y=log_df[1], name='Training Accuracy'))\n",
    "fig.add_trace(go.Scatter(x=log_df[0], y=log_df[2], name='Validation Accuracy'))\n",
    "fig.update_layout(title='Training and Validation Accuracy', xaxis_title='Epochs', yaxis_title='Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test loss\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=log_df[0], y=log_df[3], name='Training Loss'))\n",
    "fig.add_trace(go.Scatter(x=log_df[0], y=log_df[4], name='Validation Loss'))\n",
    "fig.update_layout(title='Training and Validation Loss', xaxis_title='Epochs', yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "score = model.evaluate(validation_generator, steps=nb_validation_samples // batch_size)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
