{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Size of training dataset: 7808\n",
      "Size of testing dataset: 867\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Sample Data\n",
      "LABEL: F / SENTENCE: 'Half of it is going straight to charity, another quarter going straight to scientific research, an eighth to the parkour community, a sixteenth to towards spreading information about health and...|||Find a path or suffer more.|||http://personalitycafe.com/enneagram-personality-theory-forum/85323-enneagram-type-mbti-type-compared-statistics.html yep.|||I kind of anchor on Fi and Ne makes having Ni really fun. INFP for me as they tire me out less and our views tend to align more.|||The two ESTPs I have gotten the chance to know seem to experience much more than other people who have been on the planet for the same amount of time and are quite the renaissance (wo)men.  Is this...|||I don't really have a best friend ISTP(passion-amateur group co-founder), INTJ(intellectual and various small hobbies talk), ESTP(Bro-in-law, talk about everything kind of like my INTJ friend),...|||Everyone looses their gift if they don't even consider a different perspective.|||Kansas - ISTJ|||That or if they are normally comfortable with me, such as a friend or close acquaintance, they feel the need to start talking. It's almost a trap, I've noticed for most people feel the need to expose...|||To me, your answers screamed introverted feeling. Answers 2-5, 10, 11, 14, 16, and 17 your last statement were particularly Fi-like. I'm guessing you are an intuitive and possibly and introvert...|||Could you explain your reasoning for these? I saw Mako as an F, Lin as an ES, and have Kya as an F. Never had an idea for Amon's type.|||This applies to many of these threads.|||With an INFP for over 2 years now.|||After watching tonight's episode I'm sure that Unalaq is an ENXJ. I'm not sure if it's Fe or Te at this point but the way he goes about doing and planning things seem like a Je-dom. I'm putting him...|||Parkour is my passion(but I consider it closer to a martial art than a sport). I also enjoy some running and climbing.|||I have many characters but I gravitate towards sneaky archer, Breton, and conjuration. I love doing role plays and think it's one of, if not the best way to play the game.|||ESFP seems right for Ikki. We may need Jinora to have more interactions for us to tell. Any guesses about Pema, Tenzin's wife? She said herself that she used to be very shy so I'd put I just from...|||If you don't mind, please tell me more by what you meant by this bolded part or what happened.|||I think it's fit to revive this thread seeing as the second season of Korra has started and the second episode of the season is coming up tomorrow. I'd just say beware of spoilers in new posts if you...|||I was thinking more along these lines: 83385|||Yes, a few times in friendships and other things but it was usually spurred on by the idea of not having a second chance. I've been trying to make the first move more in life as I've realized it just...|||Sorry if my wording was/is confusing or vague. Let me try to explain it better.  As for the first statement: I see the world for all it's interconnections. If you wish, visualized everything having...|||~I don't experience it as simply perceiving or creating, for me as I perceive interconnected relationships are formed and realized.   ~I don't think that I rationalize with my dominate function but...|||I think it's amusing that, in the leading position I share with an ISTP friend of mine, we both start to embrace our shadows. I Think that's been my growing point lately, embracing my shadow. We're...|||I would suggest introspection and relying on your sense of self over tests and I highly suggest looking into the cognitive functions. ISTJ is the complete opposite of INFJ.|||I definitely agree with others on the US- It's pretty good for an INFJ if you find your niche.  I say the Midwest is generally SJ with women expected to be F and men to be T. It's nice but annoying....|||Please explain|||I think my own eye movements have almost been changed because of where I was usually placed when talking to someone in normal conversations. See, when I was young I ended up getting permanent spot in...|||Judgmental, critical, somewhat narcissistic, stubborn, possessive, Fe-ishly manipulative, and I have ego issues. Take that with a grain of salt.|||Yes, very much so. I love Spanish so far.|||I have a huge folder of these types of images.|||Aquarian It was just my guess, it doesn't need that much merit. Personally, I think Se is the hardest function to describe because it is so in the moment.|||Sorry, double post because of connectivity weirdness.|||I don't know if this has been posted before or if a thread about curses would be the best place but it'll do just fine. The important part is post #79, the giant wall of text. I think most of it was...|||If anything, imo, Ni would be how objects are interconnected. If I were to follow closely to your model: Introverted Intuition: Understanding how objects are connected Extraverted Intuition:...|||Sometimes you just don't see them :ninja: Seriously, I thought I was alone in a small town but I was surprised after training for a couple months.  You can easily learn and train by yourself, you...|||82063 Stuff by Andy Day, not only do I like it because it is the stuff of my passion but that new perspective of our surroundings that it brings. This is a great example of that. All those people...|||Sorry for the quality, my relative only gave me a physical copy, it's a picture of a picture. This is my INFP girlfriend of two years and me.|||If I am with my SO I almost need physical contact in some way.|||I pretty much have a guru dream that involves my SP wannabe passion. Around people I am close to I totally put on the gypsy king face, people are just so interesting. Hahaha can't stop laughing at ...|||I agree this this post very much, I just can't shake that vibe. To me it feels like you are an INTP who strongly identifies with INFJs. I think if you want a sound answer form yourself and others we...|||I do pretty well in emergencies, I do very well compared to normal conditions in my opinion. I feel like I become the ideal version of myself, for the most part. It's hard to describe but it's like...|||I have a very close INTJ friend. The Te Fe difference is acknowledged very well and I'd say that both of our tertiary functions are well developed which helps a ton. He does not show it often but he...|||Being alone and/or doing something physical that I can naturally and reactively do without thinking or little thought.|||Pretty much this|||If I wear shoes or socks to bed and my feet are not on my bed I will wake up as if I was falling. 2/3 of the time this happens. Any other dreams that I remember(I don't remember most of my dreams...|||This one still gets me.  What I meant to say was Pass the salt but what I really said was You b****, you ruined my life|||I'm sorry, but I find them so funny because I use them for good reason. They make people uncomfortable at first but then, slowly, make people more comfortable with the idea that people are different...|||XSFJ Mother, ISTJ father, and an XNFJ sister. Yep.|||I love dark jokes, especially racists/stereotypical jokes.'\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_data = pd.read_csv(\"content/training_data.csv\")\n",
    "testing_data = pd.read_csv(\"content/testing_data.csv\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
    "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Sample Data\")\n",
    "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data.iloc[-1,0], training_data.iloc[-1,1]))\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a39bbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                              posts\n",
       "0    F  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1    T  'I'm finding the lack of me in these posts ver...\n",
       "2    T  'Good one  _____   https://www.youtube.com/wat...\n",
       "3    T  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4    T  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview of the data in the csv file, which has two columns: \n",
    "# (1)type - label of the post (2)posts - the corresponding post content\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11f9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and posts and store into List\n",
    "\n",
    "# Get the list of training data (posts)\n",
    "training_posts=training_data['posts'].tolist()\n",
    "# Get the list of corresponding labels for the training data (posts)\n",
    "training_labels=training_data['type'].tolist()\n",
    "\n",
    "# Get the list of testing data (posts)\n",
    "testing_posts=testing_data['posts'].tolist()\n",
    "# Get the list of corresponding labels for the testing data (posts)\n",
    "testing_labels=testing_data['type'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be9655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the URL from the post and compare, by experimental results, when you remove the URL from the post versus keeping the URL in the post.\n",
    "# function to remove the URL from the post\n",
    "def remove_url(post):\n",
    "    import re\n",
    "    # remove the URL from the post\n",
    "    post=re.sub(r'http\\S+', '', post)\n",
    "    return post\n",
    "\n",
    "# use the function to remove the URL from the post\n",
    "training_posts_no_url=[remove_url(post) for post in training_posts]\n",
    "testing_posts_no_url=[remove_url(post) for post in testing_posts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e490100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Before removing the URL\n",
      "Accuracy of the model: 0.7820069204152249\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "After removing the URL\n",
      "Accuracy of the model: 0.7797001153402537\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#calculate the test results\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# calculate the test results\n",
    "def calculate_test_results(training_posts, training_labels, testing_posts, testing_labels):\n",
    "    # Create the bag of words\n",
    "    count_vect = CountVectorizer()\n",
    "    # Fit the bag of words on the training data\n",
    "    training_data_features = count_vect.fit_transform(training_posts)\n",
    "    # Transform the testing data\n",
    "    testing_data_features = count_vect.transform(testing_posts)\n",
    "    # Create a Multinomial Naive Bayes classifier\n",
    "    clf = MultinomialNB().fit(training_data_features, training_labels)\n",
    "    # Predict the labels on the testing data\n",
    "    predicted_labels = clf.predict(testing_data_features)\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(testing_labels, predicted_labels)\n",
    "    return accuracy\n",
    "\n",
    "#before removing the URL\n",
    "print(\"------------------------------------\")\n",
    "print(\"Before removing the URL\")\n",
    "print(\"Accuracy of the model: {0}\".format(calculate_test_results(training_posts, training_labels, testing_posts, testing_labels)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "# after removing the URL, the accuracy of the model is higher\n",
    "print(\"------------------------------------\")\n",
    "print(\"After removing the URL\")\n",
    "print(\"Accuracy of the model: {0}\".format(calculate_test_results(training_posts_no_url, training_labels, testing_posts_no_url, testing_labels)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "# The result is that the accuracy of the model is lower when you remove the URL from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d56f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Before processing the training set\n",
      "Accuracy of the model: 0.7820069204152249\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "After processing the training set\n",
      "Accuracy of the model: 0.657439446366782\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#pre-process the training set by integrating several text pre-processing techniques (e.g. tokenisation, removing numbers, converting to lowercase, removing stop words, stemming, etc.).\n",
    "#  You should test and justify the reason why you apply the specific preprocessing techniques based on the test results.\n",
    "\n",
    "#tokenisation\n",
    "import re, string\n",
    "# function to tokenise the post\n",
    "def tokenise(post):\n",
    "    # tokenise the post\n",
    "    post=re.sub(\"[^a-zA-Z]\", \" \", post)\n",
    "    post=post.lower()\n",
    "    return post\n",
    "\n",
    "#remove numbers from the post\n",
    "def remove_numbers(post):\n",
    "    # remove the numbers from the post\n",
    "    post=re.sub(\"[0-9]\", \" \", post)\n",
    "    return post\n",
    "\n",
    "#convert to lowercase\n",
    "def convert_to_lowercase(post):\n",
    "    # convert to lowercase\n",
    "    post=post.lower()\n",
    "    return post\n",
    "\n",
    "#remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "\n",
    "# function to remove the stop words from the post\n",
    "def remove_stop_words(post):\n",
    "    # remove the stop words from the post\n",
    "    post=post.split()\n",
    "    post=[word for word in post if not word in stop_words]\n",
    "    post=\" \".join(post)\n",
    "    return post\n",
    "\n",
    "# function to remove punctuations\n",
    "def remove_punctuations(post):\n",
    "    # remove the punctuations from the post\n",
    "    post = [word for word in post if word not in string.punctuation]\n",
    "    post = ' '.join(post)\n",
    "    return post\n",
    "\n",
    "#stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "# function to stem the post\n",
    "def stem_post(post):\n",
    "    # stem the post\n",
    "    post=post.split()\n",
    "    post=[stemmer.stem(word) for word in post]\n",
    "    post=\" \".join(post)\n",
    "    return post\n",
    "\n",
    "\n",
    "# apply the tokenisation, remove numbers, convert to lowercase, remove stop words, stemming\n",
    "def preprocess_training_set(training_posts):\n",
    "    # apply the tokenisation, remove numbers, convert to lowercase, remove stop words, stemming\n",
    "    training_posts=[tokenise(post) for post in training_posts]\n",
    "    training_posts=[remove_numbers(post) for post in training_posts]\n",
    "    training_posts=[convert_to_lowercase(post) for post in training_posts]\n",
    "    training_posts=[remove_stop_words(post) for post in training_posts]\n",
    "    # training_posts = [remove_punctuations(post) for post in training_posts]\n",
    "    training_posts=[stem_post(post) for post in training_posts]\n",
    "    return training_posts\n",
    "\n",
    "processed_training_posts=preprocess_training_set(training_posts)\n",
    "\n",
    "#test the results\n",
    "print(\"------------------------------------\")\n",
    "print(\"Before processing the training set\")\n",
    "print(\"Accuracy of the model: {0}\".format(calculate_test_results(training_posts, training_labels, testing_posts, testing_labels)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "#after processing the training set\n",
    "print(\"------------------------------------\")\n",
    "print(\"After processing the training set\")\n",
    "print(\"Accuracy of the model: {0}\".format(calculate_test_results(processed_training_posts, training_labels, testing_posts, testing_labels)))\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825aad6d",
   "metadata": {},
   "source": [
    "## PART B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff8b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" build a word embedding model (for representing word vectors, such as word2vec-CBOW, word2vec-Skip gram, fastText, and Glove) \n",
    "for the input embedding of your sequence model  \"\"\"\n",
    "\n",
    "# Training word embeddings using processed_training_posts for the input embedding of your sequence model\n",
    "#start\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# training word embeddings\n",
    "def train_word_embeddings(training_posts):\n",
    "    # training word embeddings\n",
    "    model = Word2Vec(training_posts, window=5, workers=4)\n",
    "    return model\n",
    "\n",
    "#training model for word embedding\n",
    "word_embedding_model=train_word_embeddings(processed_training_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a31bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and apply the pretrained word embedding to the training set\n",
    "def extract_pretrained_word_embedding(training_posts, word_embedding_model):\n",
    "    # extract and apply the pretrained word embedding to the training set\n",
    "    training_posts_embedding = []\n",
    "    for post in training_posts:\n",
    "        post_embedding = []\n",
    "        for word in post.split():\n",
    "            try:\n",
    "                post_embedding.append(word_embedding_model[word])\n",
    "            except:\n",
    "                continue\n",
    "        if len(post_embedding) > 0:\n",
    "            training_posts_embedding.append(post_embedding)\n",
    "    return training_posts_embedding\n",
    "\n",
    "# apply the pretrained word embedding to the testing set\n",
    "def apply_pretrained_word_embedding(testing_posts, word_embedding_model):\n",
    "    # apply the pretrained word embedding to the testing set\n",
    "    testing_posts_embedding = []\n",
    "    for post in testing_posts:\n",
    "        post_embedding = []\n",
    "        for word in post.split():\n",
    "            try:\n",
    "                post_embedding.append(word_embedding_model[word])\n",
    "            except:\n",
    "                continue\n",
    "        if len(post_embedding) > 0:\n",
    "            testing_posts_embedding.append(post_embedding)\n",
    "    return testing_posts_embedding\n",
    "\n",
    "# extract and apply the pretrained word embedding to the training set\n",
    "processed_training_posts_embedding=extract_pretrained_word_embedding(processed_training_posts, word_embedding_model)\n",
    "# apply the pretrained word embedding to the testing set\n",
    "processed_testing_posts_embedding=apply_pretrained_word_embedding(testing_posts, word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f05bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import keras\n",
    "\n",
    "\n",
    "#concatenate the trained word embedding and pretrained word embedding and apply to the sequence model\n",
    "def concatenate_pretrained_word_embedding(training_posts_embedding, testing_posts_embedding):\n",
    "    # concatenate the trained word embedding and pretrained word embedding and apply to the sequence model\n",
    "    training_posts_embedding = np.array(training_posts_embedding)\n",
    "    testing_posts_embedding = np.array(testing_posts_embedding)\n",
    "    training_posts_embedding = np.concatenate((training_posts_embedding, testing_posts_embedding), axis=0)\n",
    "    return training_posts_embedding\n",
    "\n",
    "#apply the pretrained word embedding to the training set\n",
    "training_posts_embedding=extract_pretrained_word_embedding(processed_training_posts_embedding, word_embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a74f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-directional sequence model in order to classify the label (T or F)\n",
    "#start\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Bidirectional\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# function to build the bi-directional sequence model\n",
    "def build_bi_directional_sequence_model(training_posts_embedding, training_labels):\n",
    "    # build the bi-directional sequence model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(units=64)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# function to train the bi-directional sequence model\n",
    "def train_bi_directional_sequence_model(model, training_posts_embedding, training_labels):\n",
    "    # train the bi-directional sequence model\n",
    "    model.fit(training_posts_embedding, training_labels, epochs=10, batch_size=32, verbose=1)\n",
    "    return model\n",
    "\n",
    "# function to test the bi-directional sequence model\n",
    "def test_bi_directional_sequence_model(model, testing_posts_embedding, testing_labels):\n",
    "    # test the bi-directional sequence model\n",
    "    scores = model.evaluate(testing_posts_embedding, testing_labels, verbose=1)\n",
    "    return scores\n",
    "\n",
    "# function to predict the label (T or F)\n",
    "def predict_label(model, testing_posts_embedding):\n",
    "    # predict the label (T or F)\n",
    "    predicted_labels = model.predict(testing_posts_embedding)\n",
    "    predicted_labels = np.where(predicted_labels > 0.5, 1, 0)\n",
    "    return predicted_labels\n",
    "\n",
    "# function to calculate the accuracy of the bi-directional sequence model\n",
    "def calculate_bi_directional_sequence_model_accuracy(predicted_labels, testing_labels):\n",
    "    # calculate the accuracy of the bi-directional sequence model\n",
    "    accuracy = np.mean(predicted_labels == testing_labels)\n",
    "    return accuracy\n",
    "\n",
    "# function to calculate the F1 score of the bi-directional sequence model\n",
    "def calculate_bi_directional_sequence_model_f1_score(predicted_labels, testing_labels):\n",
    "    # calculate the F1 score of the bi-directional sequence model\n",
    "    f1_score = f1_score(testing_labels, predicted_labels)\n",
    "    return f1_score\n",
    "\n",
    "# function to calculate the precision of the bi-directional sequence model\n",
    "def calculate_bi_directional_sequence_model_precision(predicted_labels, testing_labels):\n",
    "    # calculate the precision of the bi-directional sequence model\n",
    "    precision = precision_score(testing_labels, predicted_labels)\n",
    "    return precision\n",
    "\n",
    "# function to calculate the recall of the bi-directional sequence model\n",
    "def calculate_bi_directional_sequence_model_recall(predicted_labels, testing_labels):\n",
    "    # calculate the recall of the bi-directional sequence model\n",
    "    recall = recall_score(testing_labels, predicted_labels)\n",
    "    return recall\n",
    "\n",
    "# build the bi-directional sequence model\n",
    "bi_directional_sequence_model = build_bi_directional_sequence_model(training_posts_embedding, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f1d880",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_posts_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-888edc2fb69c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# emantic-Syntactic word relationship tests for the trained word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msemantic_syntactic_word_relationship_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconduct_semantic_syntactic_word_relationship_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_posts_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_posts_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# apply tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "def apply_semantic_syntactic_word_relationship_tests(training_posts_embedding, training_labels):\n",
    "    # apply tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "    training_posts_embedding_tSemantic_Syntactic_word_relationship_tests = []\n",
    "    for post in training_posts_embedding:\n",
    "        post_embedding_tSemantic_Syntactic_word_relationship_tests = []\n",
    "        for word_embedding in post:\n",
    "            word_embedding_tSemantic_Syntactic_word_relationship_tests = []\n",
    "            for word_embedding_test in post:\n",
    "                word_embedding_tSemantic_Syntactic_word_relationship_tests.append(np.dot(word_embedding, word_embedding_test))\n",
    "            post_embedding_tSemantic_Syntactic_word_relationship_tests.append(word_embedding_tSemantic_Syntactic_word_relationship_tests)\n",
    "        training_posts_embedding_tSemantic_Syntactic_word_relationship_tests.append(post_embedding_tSemantic_Syntactic_word_relationship_tests)\n",
    "    return training_posts_embedding_tSemantic_Syntactic_word_relationship_tests\n",
    "\n",
    "\n",
    "# display the results of applying tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "def display_semantic_syntactic_word_relationship_tests_results(training_posts_embedding_tSemantic_Syntactic_word_relationship_tests, training_labels):\n",
    "    # display the results of applying tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "    print('\\n\\n\\n\\n')\n",
    "    print('The results of applying tSemantic-Syntactic word relationship tests for the trained word embeddings:')\n",
    "    print('\\n\\n')\n",
    "    for i in range(len(training_posts_embedding_tSemantic_Syntactic_word_relationship_tests)):\n",
    "        print('The results of applying tSemantic-Syntactic word relationship tests for the post number: ' + str(i + 1))\n",
    "        print('\\n')\n",
    "        for j in range(len(training_posts_embedding_tSemantic_Syntactic_word_relationship_tests[i])):\n",
    "            print('The results of applying tSemantic-Syntactic word relationship tests for the word number: ' + str(j + 1))\n",
    "            print('\\n')\n",
    "            print('The tSemantic-Syntactic word relationship tests results: ' + str(training_posts_embedding_tSemantic_Syntactic_word_relationship_tests[i][j]))\n",
    "            print('\\n')\n",
    "        print('\\n\\n')\n",
    "    print('\\n\\n\\n\\n')\n",
    "\n",
    "# display the results of applying tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "display_semantic_syntactic_word_relationship_tests_results(apply_semantic_syntactic_word_relationship_tests(training_posts_embedding, training_labels), training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1039f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation with Data Processing Techiques\n",
    "# apply tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "def apply_semantic_syntactic_word_relationship_tests_for_testing_posts_embedding(testing_posts_embedding):\n",
    "    # apply tSemantic-Syntactic word relationship tests for the trained word embeddings\n",
    "    testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests = []\n",
    "    for post in testing_posts_embedding:\n",
    "        post_embedding_tSemantic_Syntactic_word_relationship_tests = []\n",
    "        for word_embedding in post:\n",
    "            word_embedding_tSemantic_Syntactic_word_relationship_tests = []\n",
    "            for word_embedding_test in post:\n",
    "                word_embedding_tSemantic_Syntactic_word_relationship_tests.append(np.dot(word_embedding, word_embedding_test))\n",
    "            post_embedding_tSemantic_Syntactic_word_relationship_tests.append(word_embedding_tSemantic_Syntactic_word_relationship_tests)\n",
    "        testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests.append(post_embedding_tSemantic_Syntactic_word_relationship_tests)\n",
    "    return testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests\n",
    "\n",
    "# print the results\n",
    "def print_semantic_syntactic_word_relationship_tests_results(testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests, testing_labels):\n",
    "    # print the results\n",
    "    print('\\n\\n\\n\\n')\n",
    "    print('The results of applying tSemantic-Syntactic word relationship tests for the testing posts embeddings:')\n",
    "    print('\\n\\n')\n",
    "    for i in range(len(testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests)):\n",
    "        print('The results of applying tSemantic-Syntactic word relationship tests for the post number: ' + str(i + 1))\n",
    "        print('\\n')\n",
    "        for j in range(len(testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests[i])):\n",
    "            print('The results of applying tSemantic-Syntactic word relationship tests for the word number: ' + str(j + 1))\n",
    "            print('\\n')\n",
    "            print('The tSemantic-Syntactic word relationship tests results: ' + str(testing_posts_embedding_tSemantic_Syntactic_word_relationship_tests[i][j]))\n",
    "            print('\\n')\n",
    "        print('\\n\\n')\n",
    "    print('\\n\\n\\n\\n')\n",
    "\n",
    "# print the results\n",
    "print_semantic_syntactic_word_relationship_tests_results(apply_semantic_syntactic_word_relationship_tests_for_testing_posts_embedding(processed_testing_posts_embedding), testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6948a380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2f7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation with Different Sequence Models\n",
    "# import ConvID, GlobalMaxPoolingID\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "def performance_evaluation_with_different_sequence_models(model, testing_posts_embedding, testing_labels):\n",
    "    # performance evaluation with different sequence models\n",
    "    for dimension in [100, 200, 300]:\n",
    "        for window_size in [3, 5]:\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(len(word_embedding_model.wv.vocab), dimension, input_length=testing_posts_embedding.shape[1]))\n",
    "            model.add(Conv1D(250, window_size, padding='valid', activation='relu', strides=1))\n",
    "            model.add(GlobalMaxPooling1D())\n",
    "            model.add(Dense(250))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            score = model.evaluate(testing_posts_embedding, testing_labels, verbose=1)\n",
    "            print(\"Dimension: {0}, Window Size: {1}, Accuracy: {2}\".format(dimension, window_size, score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7020ee65",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-4a5d3cbc03b3>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4a5d3cbc03b3>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    model.add(Embedding(len(word_embedding_model.wv.vocab), dimension, input_length=))\u001b[0m\n\u001b[0m                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# HyperParameter Testing\n",
    "# import ConvID, GlobalMaxPoolingID\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "def hyper_parameter_testing(model, testing_posts_embedding, testing_labels):\n",
    "    # hyper parameter testing\n",
    "    for dimension in [100, 200, 300]:\n",
    "        for window_size in [3, 5]:\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(len(word_embedding_model.wv.vocab), dimension, input_length=))\n",
    "            model.add(Conv1D(250, window_size, padding='valid', activation='relu', strides=1))\n",
    "            model.add(GlobalMaxPooling1D())\n",
    "            model.add(Dense(250))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            score = model.evaluate(testing_posts_embedding, testing_labels, verbose=1)\n",
    "            print(\"Dimension: {0}, Window Size: {1}, Accuracy: {2}\".format(dimension, window_size, score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d682136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Personality Type Prediction\n",
    "\n",
    "text = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# design a user interface so that user can input a textual sentence via the colab form fields user interface to get the personality type classification result from your trained model\n",
    "def get_personality_type_prediction(text):\n",
    "    # get the personality type classification result from your trained model\n",
    "    return \"I am a \" + text\n",
    "\n",
    "# print the result\n",
    "print(get_personality_type_prediction(text))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13c4fa65054c0d3bfb9dbdc50fbf8af2a6ced466e99fd692bb576917c08ad093"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
